#  PakWheels ETL Pipeline â€“ Scalable Web Scraping & Data Processing

This project builds an end-to-end ETL pipeline to scrape, clean, and process vehicle listings from **PakWheels.com**, using a modern data engineering stack. It combines **Selenium** for web scraping, **PySpark** for big data processing, **Apache Airflow** for task orchestration, and **Docker** for containerization.

The goal: turn raw, unstructured web data into clean, analysis-ready datasets for automotive insights and modeling.


##  Tech Stack

-  **Selenium** â€“ For dynamic scraping of vehicle listings  
-  **Apache Airflow** â€“ For scheduling and orchestrating ETL tasks  
-  **PySpark** â€“ For scalable data transformation and cleaning  
-  **Docker** â€“ For containerizing the entire workflow  
-  **CSV / Parquet Output** â€“ Clean datasets for downstream use

---

##  Workflow Overview

1. **Scraping:**  
   Selenium script dynamically scrapes car listing data from PakWheels (price, brand, year, engine, mileage, location, etc.)

2. **ETL Scheduling:**  
   Airflow DAG schedules the scraping, transformation, and loading tasks to run on a defined frequency.

3. **Transformation:**  
   PySpark processes and cleans the raw dataâ€”handling nulls, types, price formats, duplicate listings, etc.

4. **Export:**  
   Final cleaned dataset is saved in CSV/Parquet format for use in dashboards or ML models.

---

## ğŸ” Key Features

- Scrapes real-time listings from PakWheels using Selenium  
- Cleans and structures messy scraped data using PySpark  
- Task automation with Airflow DAGs (fully scheduled + monitored)  
- Dockerized pipeline for easy deployment and scalability  
- Output ready for dashboarding, analytics, or machine learning

---

##  Folder Structure
pakwheels-etl/
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/
â”‚ â””â”€â”€ pakwheels_etl_dag.py
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ scraper/
â”‚ â””â”€â”€ pakwheels_scraper.py
â”œâ”€â”€ spark_jobs/
â”‚ â””â”€â”€ clean_transform.py
â””â”€â”€ output/
â””â”€â”€ cleaned_data.csv


---

##  How to Run

1. Clone the repo  
2. Build Docker containers  
3. Launch Airflow with Docker Compose  
4. Trigger DAG or schedule run  
5. Cleaned data will appear in `/output` folder

---

##  Sample Use Cases

- Build a Power BI dashboard on market trends by city, brand, price range  
- Train an ML model to predict resale value based on features  
- Analyze market demand for different car types over time

---

##  Contact

**Muhammad Abdullah Sheikh**  
ğŸ“§ abdullahsheeikh.638@gmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/muhammad-abdullah-sheikh) | [GitHub](https://github.com/yk-Abdullah)


